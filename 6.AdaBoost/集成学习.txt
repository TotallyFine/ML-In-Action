1 什么是元算法/集成学习？
  对其他的算法进行组合，将不同的分类器进行组合或者不同参数下的同一分类器进行组合。集成学习的基本思想是弱分类器也有超过50%的性能，将他们集成起来就可以得到较好的效果。
2 boosting算法是什么？
  不同分离齐齐串行训练，集中关注被已有分类器错分的那些数据来获得新的分类器，不同分类器的权重不同。
3 如何处理非均衡分类问题？
  非均衡分类问题是，当对样例数目不均衡的数据进行分类。可能需要利用修改后的指标来评价分类器的性能。
4 什么是bagging算法？
  自举汇聚法 bootstrap aggrefating，对原始数据选择S次得到S个子训练集，训练出S个分类器，不同的分类器间的权重相同。
5 为什么AdaBoost算法的效果很好？
  算法是集成了不同的弱分类器，算法容易实现，且通过串行的训练不同的弱分类器，并改变数据的权重，使得他们关注于上一个分类器容易出错的地方。
6 集成学习的优缺点？
  多个分类器组合可能会进一步突出单分类器的问题，比如过拟合问题。但是如果分类器之间差别显著，那么多个分类器组合就可能会缓解这一问题。分类器之间的差别可以是算法本身或者是参数不同。