1 什么是线性回归？
  线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来输出。例如y=a*x1+b*x2。而非线性回归则不认同这个做法，他们认为输出可能是输入的乘积或其他形式，比如y=a*x1/x2。
2 在线性回归中如何求最佳的权重系数？
  使用平方误差来度量模型，∑(yi - xi.T*W)，对W求导令倒数等于零得到W=(X.T*X)的逆矩阵*X.T*y。上式需要对矩阵求逆，所以需要逆矩阵存在。
  如果逆矩阵不存在则使用普通最小二乘法Ordinary Least Squares
3 如何判断拟合直线的效果？
  同时计算yHat序列和真实只y序列的匹配程度，即计算相关系数。首先计算得到yHat=xMat*ws, 然后使用numpy中的np.corrcoef(yHat.T, yMat),得到一个2X2的矩阵，对角线上的数字是1因为自己和自己匹配程度最高，然后其余两个角上就是相关系数。
4 线性回归为什么欠拟合很严重？
  因为它求的是具有最小均方误差的无偏估计。
5 什么是局部加权线性回归？
  Locally Weighted Linear Regression LWLR。在线性回归的基础上引入一些偏差，从而降低预测的均方误差。给待预测点附近的每个店赋予了一定的权重，然后在这个子集上进行普通的回归。所以系数为w=(xTWx)-1XTWy。LWLR常使用核（与支持向量机中的核类似）来对附近的点赋予更高的权重，常用的是高斯核。
6 局部加权线性回归中的平滑系数是什么？
  平滑系数决定了对训练集附近的点赋予多大权重。使用高斯核，当平滑系数较小的时候曲线越贴近训练集的点越容易过拟合，当平滑系数越大则越形成直线越容易欠拟合。
7 什么是岭回归？它适用于什么情况？
  岭回归ridge regression，解决特征比样本点还多的情况，现在也用于在估计中加入偏差，从而得到更好的估计。岭回归就是在矩阵xTx上加一个lambdaI，lambda是一个系数，I是一个mXm的单位矩阵。这里通过加入lambda来限制所有w之和，通过引入惩罚项能减少不重要的参数，这种技术在统计学上叫做缩减。岭回归中的岭就是I中的斜对角全1的那条线。
8 岭回归可以解释回归系数么？
  可以，通过设置不同的缩减系数得到不同的权重，在整个权重的变化中就可以得到权重的变化情况，其中权重的绝对值越大则影响力越大。
9 什么是lasso？
  一种回归方法，将权重的绝对值之和限制小于lambda，而岭回归则是将权重的平方和限制小于lambda。使用lasso在lambda足够小的时候一些系数会因此被迫缩减到0。一般使用前向逐步回归来实现lasso。
10 如何使用前向逐步回归实现lasso？
  前向逐步回归是一种贪心算法，每步都尽可能减少误差。一开始，所有权重都设为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值。需要对数据进行归一化。
11 逐步线性回归的好处是什么？
  可以帮助人们理解并改进模型，当构建一个模型后，可以运行该算法找出重要的特征，这样可以及时停止对不重要特征的收集。可以使用类似于10折交叉雁阵的方法比较这些模型，最终选择使误差最小的模型。